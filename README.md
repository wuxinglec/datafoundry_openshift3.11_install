#OpenShift 3.11 Centos 集群安装
##准备你的主机(host)
### SELinux要求
在安装OpenShift Container Platform之前，必须在所有服务器上启用安全增强型Linux（SELinux），否则安装程序将失败。另外，在/etc/selinux/config文件中配置SELINUX = enforcing和SELINUXTYPE = targeted：

```
# This file controls the state of SELinux on the system.
# SELINUX= can take one of these three values:
#     enforcing - SELinux security policy is enforced.
#     permissive - SELinux prints warnings instead of enforcing.
#     disabled - No SELinux policy is loaded.
SELINUX=enforcing
# SELINUXTYPE= can take one of these three values:
#     targeted - Targeted processes are protected,
#     minimum - Modification of targeted policy. Only selected processes are protected.
#     mls - Multi Level Security protection.
SELINUXTYPE=targeted
```
### DNS要求
OpenShift Container Platform需要环境中功能齐全的DNS服务器。理想情况下，这是运行DNS软件的独立主机，可以为平台上运行的主机和容器提供名称解析。

**注意：在每个主机上的/ etc / hosts文件中添加条目是不够的。此文件不会复制到平台上运行的容器中。**
OpenShift Container Platform的关键组件在容器内部运行，并使用以下过程进行名称解析：

1. 默认情况下，容器从其主机接收其DNS配置文件（/etc/resolv.conf）
2. 然后，OpenShift Container Platform将pod的第一个名称服务器设置为节点的IP地址。

从OpenShift Container Platform 3.2开始，dnsmasq将自动配置在所有主服务器和节点上。 pod使用节点作为DNS，节点转发请求。默认情况下，在节点上配置dnsmasq以侦听端口53，因此节点无法运行任何其他类型的DNS应用程序。

**注：** NetworkManager是一个为系统提供检测和配置以自动连接到网络的程序，在节点上需要使用DNS IP地址填充dnsmasq。默认情况下，NM_CONTROLLED设置为yes。如果NM_CONTROLLED设置为no，则NetworkManager调度脚本不会创建相关的origin-upstream-dns.conf dnsmasq文件，您必须手动配置dnsmasq。同样，如果网络脚本中的PEERDNS参数设置为no，例如/etc/sysconfig/network-scripts/ifcfg-em1，则不会生成dnsmasq文件，并且Ansible安装将失败。确保PEERDNS设置为yes。

以下是一组DNS记录示例：

```
master1    A   10.64.33.100
master2    A   10.64.33.103
node1      A   10.64.33.101
node2      A   10.64.33.102
```

如果您没有正常运行的DNS环境，则可能会遇到以下问题：

- 通过引用基于Ansible的脚本进行产品安装
- 部署基础架构容器（注册表，路由器）
- 访问OpenShift Container Platform Web控制台，因为它无法单独通过IP地址访问

### 配置主机使用DNS
确保环境中的每个主机都配置为从DNS服务器解析主机名。主机DNS解析的配置取决于是否启用了DHCP。如果DHCP是：
	
- 禁用，然后将网络接口配置为静态，并将DNS名称服务器添加到NetworkManager
- 启用，NetworkManager调度脚本会根据DHCP配置自动配置DNS

**要验证DNS服务器是否可以解析主机：**

1 检查/etc/resolv.conf的内容：

```
$ cat /etc/resolv.conf
# Generated by NetworkManager
search example.com
nameserver 10.64.33.1
# nameserver updated by /etc/NetworkManager/dispatcher.d/99-origin-dns.sh

注：10.64.33.1是我们的DNS服务器的地址。
```
2 测试/etc/resolv.conf中列出的DNS服务器是否能够将主机名解析为OpenShift Container Platform环境中所有主服务器和节点的IP地址：
 
```
$ dig master.example.com @10.64.33.1 +short
10.64.33.100
$ dig node1.example.com @10.64.33.1 +short
10.64.33.101
```  
###配置DNS WILDCARD
(可选）,为路由器配置通配符，以便在添加新路由时不需要更新DNS配置。DNS区域的通配符必须最终解析为OpenShift Container Platform路由器的IP地址。

例如，为具有较低生存时间值（TTL）的cloudapps创建通配符DNS条目，并指向将部署路由器的主机的公共IP地址：

```
*.cloudapps.example.com. 300 IN  A 192.168.133.2
```
**注意：在每个节点主机上的/etc/resolv.conf文件中，确保具有通配符条目的DNS服务器未列为名称服务器，或者未在搜索列表中列出通配符域，否则，OpenShift Container Platform管理的容器可能无法正确解析主机名。**

###配置节点主机名称
当你设置一个非云供应商提供的集群时，你必须正确的配置你node节点的主机名。每个节点的主机名必须是可解析的，并且每个节点必须能够访问每个其他节点。
确认一个节点可以到达领一个节点：

1 在一个节点获取主机名
 
```
$ hostname

master-1.example.com
```

2 在同一节点上，获取主机的完全限定域名：

```
$ hostname -f

master-1.example.com
```

3 在另一个节点上，确认你可以访问第一个节点

```
$ ping master-1.example.com -c 1

PING master-1.example.com (172.16.122.9) 56(84) bytes of data.
64 bytes from master-1.example.com (172.16.122.9): icmp_seq=1 ttl=64 time=0.319 ms

--- master-1.example.com ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.319/0.319/0.319/0.000 ms
```

### 网络访问要求
主节点主机和节点主机之间必须存在共享网络，如果计划使用标准群集安装过程为高可用性配置多​​个主服务器，则还必须在安装过程中选择要配置为虚拟IP（VIP）的IP。您选择的IP必须可在所有节点之间路由，如果使用FQDN进行配置，则必须在所有节点上解析。

###NETWORKMANAGER(网络管理员)
NetworkManager是一个为系统提供检测和配置以自动连接到网络的程序，在节点上需要使用DNS IP地址填充dnsmasq。默认情况下，NM_CONTROLLED设置为yes。如果NM_CONTROLLED设置为no，则NetworkManager调度脚本不会创建相关的origin-upstream-dns.conf dnsmasq文件，您必须手动配置dnsmasq。

###配置防火墙
虽然iptables是默认防火墙，但建议将firewalld用于新安装。您可以通过在[Ansible清单文件](https://docs.openshift.com/container-platform/3.11/install/configuring_inventory_file.html#advanced-install-configuring-firewalls)中设置os_firewall_use_firewalld = true来启用firewalld。

```
[OSEv3:vars]
os_firewall_use_firewalld=True
```
将此变量设置为true将打开所需的端口并将规则添加到默认区域，从而确保正确配置firewalld。
###需要的端口
OpenShift Container Platform安装使用iptables在每个主机上自动创建一组内部防火墙规则。然而如果你的网络配置使用一个外部的防火墙，例如hardware-based防火墙，你必须确保基础架构组件可以通过充当某些进程或服务的通信端点的特定端口相互通信。

确保OpenShift Container Platform所需的以下端口在网络上打开，并配置为允许主机之间的访问。根据您的配置和使用情况，某些端口是可选的。
[端口详情](https://docs.openshift.com/container-platform/3.11/install/prerequisites.html#wildcard-dns-prereq)

### 确保主机访问

OpenShift容器平台安装要求一个用户可以访问所有的主机。如果你想用非root用户安装请先配置无密码sudo权限。
	
1. 在你的主机上生成一个SSH key

	ssh-keygen
	
	不输入密码
	
2. 分发key到集群的其他主机，bsah脚本
	 
	 	# for host in master.example.com \        `1`
    	node1.example.com \                    `1`  
    	node2.example.com; \                   `1` 
    	do ssh-copy-id -i ~/.ssh/id_rsa.pub $host; \
    	done
	 
 注：请将`1`替换为每台机器的主机名

3. 确认你可以通过SSH访问所有集群内的主机
   
###设置代理

  /ett/environment，
  设置no_proxy值，允许OpenShift容器平台组件可以开放的通信，如果既需要http_proxy或者https_proxy的值，可按如下示例修改
  ```
  no_proxy=.internal.example.com,10.0.0.1,10.0.0.2,10.0.0.3,.cluster.local,.svc,localhost,127.0.0.1,172.30.0.1
  ```
  
###注册hosts

   为了能够访问安装包，您必须使用Red Hat Subscription Manager（RHSM）注册每个主机并附加有效的OpenShift Container Platform订阅。
   
1. 在每一个host上注册RHSM
    
    	# subscription-manager register --username=<user_name> --password=<password>
    
    
2. 从RHSM中提取最新的订阅数据：
    
    	# subscription-manager refresh
    
3. 列出可用的订阅：  
    	# subscription-manager list --available --matches '*OpenShift*'
    
4. 在上一个命令的输出中，找到OpenShift Container Platform订阅的池ID并附加它：

		#subscription-manager attach --pool=<pool_id>
5. 禁用所有yum存储库

	```
	a 禁用所有已启用的RHSM存储库：
		# subscription-manager repos --disable="*"
	b 列出剩余的yum存储库，并在repo id下记下它们的名称（如果有）
		# yum repolist
	c 使用yum-config-manager禁用剩余的yum存储库：
		# yum-config-manager --disable <repo_id>
	  或者，禁用所有存储库：
		yum-config-manager --disable \*
	请注意，如果您拥有大量可用存储库，则可能需要几分钟时间 
	
	```
	
6. 仅启用OpenShift Container Platform 3.11所需的存储库

		# subscription-manager repos \
   		--enable="rhel-7-server-rpms" \
   	 	--enable="rhel-7-server-extras-rpms" \
    	--enable="rhel-7-server-ose-3.11-rpms" \
    	--enable="rhel-7-server-ansible-2.6-rpms"
    	
###安装基础包

**注意：**如果您的主机使用RHEL 7.5，并且您希望接受OpenShift Container Platform的默认docker配置（使用OverlayFS存储和所有默认日志记录选项），请不要手动安装这些软件包。如果主机使用RHEL 7.4或者​​如果它们使用RHEL 7.5并且您想要自定义docker配置，请安装这些软件包。

**For RHEL 7 systems:**

	1 安装下面的基础包
		# yum install wget git net-tools bind-utils yum-utils iptables-services bridge-utils bash-completion kexec-tools sos psacct
	2 将系统更新到最新的包：
		# yum update
		# reboot
	3 安装安装方法所需的软件包：
		a 如果您打算使用容器化安装程序，请安装以下软件包：
			# yum install atomic
		b 如果您计划使用基于RPM的安装程序，请安装以下软件包：
			# yum install openshift-ansible
			此程序包提供安装程序实用程序并提取群集安装过程所需的其他程序包，例如Ansible，playbooks和相关配置文件

			
**For RHEL Atomic Host 7 systems:**

	1 通过升级到最新的Atomic树（如果有），确保主机是最新的：
		# atomic host upgrade
	2 升级完成后重启
		# reboot
###安装Docker

安装docker到所有的master与node主机上。允许在在安装OpenShift Container Platform(OpenShift容器平台)之前配置你的Docker storage options(Docker存储选项)

**注：集群安装过程会自动的修改/etc/sysconfig/docker文件**

**For RHEL 7 systems:**
	
	a 安装Docker 1.13
		# yum install docker-1.13.1
	b 验证是否已安装版本1.13：
		# rpm -V docker-1.13.1
		# docker version
		
**For RHEL Atomic Host 7 systems:**

无需任何操作。默认情况下，Docker已安装，配置和运行。

###配置Docker存储

容器及其创建的镜像存储在Docker的存储后端。此存储是短暂的，与分配的任何持久存储分开，以满足您的应用程序的需要。使用Ephemeral存储时，容器保存的数据会在删除容器时丢失。对于持久存储，如果删除容器，则容器保存的数据仍然存在。

你必须配置所有master与node节点的存储，因为系统会默认运行一个容器守护进程。对于容器化安装，您需要在master(主机)上存储。此外，默认情况下，需要存储的Web控制台和etcd在主服务器的容器中运行。容器在节点(node)上运行，因此始终需要存储。

存储大小取决于工作负载，容器数量，正在运行的容器的大小以及容器的存储要求。

**注意：如果您的主机使用RHEL 7.5，并且您希望接受OpenShift Container Platform的默认docker配置（使用OverlayFS存储和所有默认日志记录选项），请不要手动安装这些软件包。如果主机使用RHEL 7.4或者​​如果它们使用RHEL 7.5并且您想要自定义docker配置，请安装这些软件包。**

**For RHEL 7 systems:**

RHEL 7上Docker的默认存储后端是环回设备上的精简池，不支持生产使用，仅适用于概念验证环境。对于生产环境，您必须创建精简池逻辑卷并重新配置Docker以使用该卷。

Docker将图像和容器存储在图形驱动程序中，图形驱动程序是一种可插拔的存储技术，例如DeviceMapper，OverlayFS和Btrfs。每个都有优点和缺点。例如，OverlayFS在启动和停止容器时比DeviceMapper快，但由于union文件系统的体系结构限制，因此不符合Unix（POSIX）的可移植操作系统接口。有关在您的RHEL版本中使用OverlayFS的信息，请参阅Red Hat Enterprise Linux发行说明。

**For RHEL Atomic Host 7 systems:**

RHEL Atomic Host上Docker的默认存储后端是精简池逻辑卷，适用于生产环境。您必须确保根据系统要求中提到的Docker存储要求为此卷分配足够的空间。

###配置 OverlayFS

OverlayFS是一种联合文件系统。它允许您将一个文件系统覆盖在另一个文件系统之上。更改记录在上部文件系统中，而下部文件系统保持不变。

###配置精简池存储

您可以使用Docker附带的docker-storage-setup脚本创建精简池设备并配置Docker的存储驱动程序。您可以在安装Docker之后执行此操作，并且必须在创建镜像或容器之前执行此操作。该脚本从/ etc / sysconfig / docker-storage-setup文件中读取配置选项，并支持三个用于创建逻辑卷的选项：

	1. 使用其它块设备
	2. 使用现有的指定卷组
	3. 使用根文件系统所在的卷组中的剩余可用空间
	
使用其它块设备是一个很好的选择，但是它要去在配置你的Docker存储之前先添加块设备。其它的选项都需要在你配置主机时预留出空间。使用第三种选项会对一些应用产生一些影响，例如红帽的RHMAP。

1.  使用以下三个选项之一创建docker-pool卷：	
	- 使用额外的块设备

		a. /etc/sysconfig/docker-storage-setup，将DEVS设置为要使用的块设备的路径。将VG设置为要创建的卷组名称，例如docker-vg。例如：
			
			# cat <<EOF > /etc/sysconfig/docker-storage-setup
			DEVS=/dev/vdc
			VG=docker-vg
			EOF
		b.运行docker-storage-setup并查看输出以确保创建了docker-pool卷：
		
			# docker-storage-setup                                                                                                                                                                                                                                
			0
			Checking that no-one is using this disk right now ...
			OK

			Disk /dev/vdc: 31207 cylinders, 16 heads, 63 sectors/track
			sfdisk:  /dev/vdc: unrecognized partition table type

			Old situation:
			sfdisk: No partitions found

			New situation:
			Units: sectors of 512 bytes, counting from 0
			
   			Device Boot    Start       End   #sectors  Id  System
			/dev/vdc1          2048  31457279   31455232  8e  Linux LVM
			/dev/vdc2             0         -          0   0  Empty
			/dev/vdc3             0         -          0   0  Empty
			/dev/vdc4             0         -          0   0  Empty
			Warning: partition 1 does not start at a cylinder boundary
			Warning: partition 1 does not end at a cylinder boundary
			Warning: no primary partition is marked bootable (active)
			This does not matter for LILO, but the DOS MBR will not boot this disk.
			Successfully wrote the new partition table

			Re-reading the partition table ...

			If you created or changed a DOS partition, /dev/foo7, say, then use dd(1)to zero the first 512 bytes:  dd if=/dev/zero of=/dev/foo7 bs=512 count=1 (See fdisk(8).)
  			Physical volume "/dev/vdc1" successfully created
  			Volume group "docker-vg" successfully created
  			Rounding up size to full physical extent 16.00 MiB
 			Logical volume "docker-poolmeta" created.
  			Logical volume "docker-pool" created.
  			WARNING: Converting logical volume docker-vg/docker-pool and docker-vg/docker-poolmeta to pool's data and metadata volumes.
  			THIS WILL DESTROY CONTENT OF LOGICAL VOLUME (filesystem etc.)
  			Converted docker-vg/docker-pool to thin pool.
  			Logical volume "docker-pool" changed.
  			
  
	- 使用现有的指定卷组：
	
		a. 在/etc/sysconfig/docker-storage-setup中，将VG设置为卷组。例如
			
			#cat <<EOF > /etc/sysconfig/docker-storage-setup
			 VG=docker-vg
			 EOF
 		b. 然后运行docker-storage-setup并查看输出以确保创建了docker-pool卷：
 		
 			#docker-storage-setup
  			Rounding up size to full physical extent 16.00 MiB
  			Logical volume "docker-poolmeta" created.
  			Logical volume "docker-pool" created.
  			WARNING: Converting logical volume docker-vg/docker-pool and docker-vg/docker-poolmeta to pool's data and metadata volumes.
  			THIS WILL DESTROY CONTENT OF LOGICAL VOLUME (filesystem etc.)
  			Converted docker-vg/docker-pool to thin pool.
  			Logical volume "docker-pool" changed.
  			
  	- 使用根文件系统所在的卷组中的剩余可用空间，请执行以下操作：
  		
  		a.验证根文件系统所在的卷组是否具有所需的可用空间，然后运行docker-storage-setup并查看输出以确保创建了docker-pool卷：
  		
  			# docker-storage-setup
  			  Rounding up size to full physical extent 32.00 MiB
  			  Logical volume "docker-poolmeta" created.
  			  Logical volume "docker-pool" created.
  			  WARNING: Converting logical volume rhel/docker-pool and rhel/docker-poolmeta to pool's data and metadata volumes.
  			  THIS WILL DESTROY CONTENT OF LOGICAL VOLUME (filesystem etc.)
  			  Converted rhel/docker-pool to thin pool.
  			  Logical volume "docker-pool" changed.
  			  
2. 校验配置。确认/etc/sysconfig/docker-storage文件有dm.thinpooldev与docker-pool逻辑卷值：
		
		#cat /etc/sysconfig/docker-storage
		DOCKER_STORAGE_OPTIONS="--storage-driver devicemapper --storage-opt dm.fs=xfs --storage-opt dm.thinpooldev=/dev/mapper/rhel-docker--pool --storage-opt dm.use_deferred_removal=true --storage-opt dm.use_deferred_deletion=true " 
		#lvs
  		LV          VG   Attr       LSize  Pool Origin Data%  Meta%  Move Log 		Cpy%Sync Convert
  		docker-pool rhel twi-a-t---  9.29g             0.00   0.12
  		
  **注意：在使用Docker或OpenShift Container Platform之前，请验证docker-pool逻辑卷是否足以满足您的需求。使docker-pool卷成为可用卷组的60％;它将通过LVM监控增长以填充卷组。**
  
3. 重启Docker
	
	- 如果Docker未开启则开启，并验证
	
			# systemctl enable docker
			# systemctl start docker
			# systemctl is-active docker
	- 如果已经开启
		
		a. 重新初始化Docker,这将破坏主机上当前的任何容器或图像。
		
			# systemctl stop docker
			# rm -rf /var/lib/docker/*
			# systemctl restart docker
			
		b. 删除/var/lib/docker/文件夹中的所有内容。
		
###重新配置Docker存储
如果在创建docker-pool后需要重新配置Docker存储：

1. 删除docker-pool逻辑卷
2. 如果你使用专用的卷组，请删除卷组和任何关联的物理卷。
3. 在一次运行docker-storage-setup

[获取LVM管理更多信息](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/logical_volume_manager_administration/) 
		
###启用镜像签名支持
OpenShift Container Platform能够以加密方式验证图像是否来自可靠来源。 [“容器安全指南”](https://docs.openshift.com/container-platform/3.11/security/deployment.html#security-deployment-from-where-images-deployed)提供了图像签名工作方式的高级描述。

您可以使用原子命令行界面（CLI）1.12.5或更高版本配置映像签名验证。原子CLI预安装在RHEL Atomic Host系统上。[atomic CLI详细信息](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html/cli_reference/prerequisites)

以下文件和目录包含主机的信任配置：

- /etc/containers/registries.d/*

- /etc/containers/policy.json

例如，您可以直接在每个节点上管理信任配置，或者在单独的主机上管理文件，使用Ansible将它们分配到相应的节点。

1 如果原子包未安装在主机系统上，请安装原子包：

```
$ yum install atomic
``` 

2 查看当前的信任配置：

```
$ atomic trust show
* (default)                         accept
```
默认配置是将所有注册表列入白名单，也就是说未配置签名验证。

3 自定义您的信任配置。在以下示例中，您将一个注册表或命名空间列入白名单，黑名单（拒绝）不受信任的注册表，并要求在供应商注册表上进行签名验证：

```
$ atomic trust add --type insecureAcceptAnything 172.30.1.1:5000

$ atomic trust add --sigstoretype atomic \
  --pubkeys pub@example.com \
  172.30.1.1:5000/production

$ atomic trust add --sigstoretype atomic \
  --pubkeys /etc/pki/example.com.pub \
  172.30.1.1:5000/production

$ atomic trust add --sigstoretype web \
  --sigstore https://access.redhat.com/webassets/docker/content/sigstore \
  --pubkeys /etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release \
  registry.redhat.io

# atomic trust show
* (default)                         accept
172.30.1.1:5000                     accept
172.30.1.1:5000/production          signed security@example.com
registry.redhat.io                  signed security@redhat.com,security@redhat.com
```

4 您可以通过添加全局拒绝默认信任来进一步强化节点：

```
$ atomic trust default reject

$ atomic trust show
* (default)                         reject
172.30.1.1:5000                     accept
172.30.1.1:5000/production          signed security@example.com
registry.redhat.io                  signed security@redhat.com,security@redhat.com
```

###管理容器日志
为了防止节点上运行的容器的日志文件(/var/lib/docker/containers/<hash>/<hash>-json.log)增长的过大，您可以配置Docker的json文件日志记录驱动程序来限制日志文件的大小和数量。

**选项**|**目的**|
--------|-------------------|
|--log-opt max-size|新的日志文件创建时设置它的大小|
|--log-opt max-file|设置每个主机要保留的最大日志文件数|

1 编辑/etc/sysconfig/docker文件来配置日志文件，例如，将最大文件大小设置为1 MB并始终保留最后三个日志文件，确保值保持单引号格式：

```
OPTIONS='--insecure-registry=172.30.0.0/16 --selinux-enabled --log-opt max-size=1M --log-opt max-file=3'
```

2 重启Docker服务

	# systemctl restart docker
	
###查看可用容器日志
当容器运行时你可以在/var/lib/docker/containers/<hash>/路径下查看容器日志，例如：

	# ls -lh /var/lib/docker/containers/f088349cceac173305d3e2c2e4790051799efe363842fdab5732f51f5b001fd8/
	total 2.6M
	-rw-r--r--. 1 root root 5.6K Nov 24 00:12 config.json
	-rw-r--r--. 1 root root 649K Nov 24 00:15 	f088349cceac173305d3e2c2e4790051799efe363842fdab5732f51f5b001fd8-json.log
	-rw-r--r--. 1 root root 977K Nov 24 00:15 	f088349cceac173305d3e2c2e4790051799efe363842fdab5732f51f5b001fd8-json.log.1
	-rw-r--r--. 1 root root 977K Nov 24 00:15 	f088349cceac173305d3e2c2e4790051799efe363842fdab5732f51f5b001fd8-json.log.2
	-rw-r--r--. 1 root root 1.3K Nov 24 00:12 hostconfig.json
	drwx------. 2 root root    6 Nov 24 00:12 secrets
	
###阻止本地卷使用
当在DockerFile中使用VOLUME指令或者直接使用docker run -v <volumename>命令时，将会使用主机的存储空间。使用此存储可能会导致意外的空间问题，并可能导致主机崩溃。

在OpenShift Container Platform中，尝试运行自己的映像的用户可能会在节点主机上填满整个存储空间。一种解决方法时阻止用户带卷运行他们的镜像。这样可以限制用户可以访问的唯一存储，并且集群管理员可以分配存储配额。

使用docker-novolume-plugin通过禁止启动定义了本地卷的容器来解决此问题。特别是，插件阻止了包含以下内容的docker run命令：

- --volumes-from选项
- 已定义VOLUME的镜像
- 引用使用docker volume命令配置的现有卷

插件不会阻止对绑定挂载的引用。

**要启用docker-novolume-plugin，请在每个节点主机上执行以下步骤：**

```
1 安装docker-novolume-plugin包
	$ yum install docker-novolume-plugin
2 启用并启动docker-novolume-plugin服务：
	$ systemctl enable docker-novolume-plugin
	$ systemctl start docker-novolume-plugin
3 编辑/etc/sysconfig/docker文件并将以下内容附加到OPTIONS列表：
	--authorization-plugin=docker-novolume-plugin
4 重启Docker服务：
	$ systemctl restart docker
```

###红帽Gluster存储软件要求
要访问GlusterFS卷，必须在所有可调度节点上提供mount.glusterfs命令。对于基于RPM的系统，必须安装glusterfs-fuse软件包：

	# yum install glusterfs-fuse
	
此软件包安装在每个RHEL系统上。但是，如果您的服务器使用x86_64架构，建议从Red Hat Gluster Storage更新到最新的可用版本。为此，必须启用以下RPM存储库：

	# subscription-manager repos --enable=rh-gluster-3-client-for-rhel-7-server-rpms
	
如果节点上已安装glusterfs-fuse，请确保安装了最新版本：

	# yum update glusterfs-fuse
	
##配置库存文件
###配置集群变量
要在Ansible安装期间分配全局集群环境变量，请将它们添加到/ etc / ansible / hosts文件的[OSEv3：vars]部分。您必须将每个参数值放在单独的行上。例如：

```
[OSEv3:vars]

openshift_master_identity_providers=[{'name': 'htpasswd_auth',
'login': 'true', 'challenge': 'true',
'kind': 'HTPasswdPasswordIdentityProvider',}]

openshift_master_default_subdomain=apps.test.example.com
```
**注意：如果Ansible清单文件中的参数值包含特殊字符，例如＃，{或}，则必须对该值进行双重转义（将单值和双引号中的值括起来），例如，要将mypasswordwith ### hashsigns用作变量openshift_cloudprovider_openstack_password的值，请在Ansible主机清单文件中将其声明为openshift_cloudprovider_openstack_password ='“mypasswordwith ### hashsigns”**

[Ansible安装程序一起使用的全局集群变量，详细信息](https://docs.openshift.com/container-platform/3.11/install/configuring_inventory_file.html)

###配置部署类别
整个Playbooks中使用的各种默认值和安装程序使用的角色基于部署类型配置（通常在Ansible清单文件中定义），确保清单文件的[OSEv3：vars]部分中的openshift_deployment_type参数设置为openshift-enterprise以安装OpenShift Container Platform变体：

```
[OSEv3:vars]
openshift_deployment_type=openshift-enterprise
```

###配置主机变量
要在Ansible安装期间将环境变量分配给主机，请在[masters]或[nodes]部分中的主机条目之后将其设置在/ etc / ansible / hosts文件中。例如:

```
[masters]
ec2-52-6-179-239.compute-1.amazonaws.com openshift_public_hostname=ose3-master.public.example.com
```
[Ansible安装程序一起使用的变量，这些变量可以分配给各个主机条目](https://docs.openshift.com/container-platform/3.11/install/configuring_inventory_file.html)

###定义节点组和主机映射
节点配置从主服务器引导。当节点引导并启动服务时，节点会在加入群集之前检查是否存在kubeconfig和其他节点配置文件。如果不是，则节点从主服务器提取配置，然后加入群集。

此过程取代了必须在每个节点主机上唯一地手动维护节点配置的管理员。相反，节点主机的/etc/origin/node/node-config.yaml文件的内容现在由来自主服务器的ConfigMaps提供。

###节点ConfigMaps
用于定义节点配置的Configmaps必须在openshift-node项目中可用。ConfigMaps现在也是节点标签的权威定义;旧的openshift_node_labels值被有效忽略。

默认情况下，在群集安装期间，安装程序会创建以下默认ConfigMaps：

- node-config-master
- node-config-infra
- node-config-compute

还会创建以下ConfigMaps，将节点标记为多个角色：

- node-config-all-in-one
- node-config-master-infra

以下ConfigMaps是每个现有默认节点组的CRI-O变体：

- node-config-master-crio 
- node-config-infra-crio
- node-config-all-in-one-crio
- node-config-master-infra-crio

**注意：您不得修改节点主机的/etc/origin/node/node-config.yaml文件。任何更改都会被节点使用的ConfigMap中定义的配置覆盖。**

###节点组定义
安装最新的openshift-ansible软件包后，您可以在/usr/share/ansible/openshift-ansible/roles/openshift_facts/defaults/main.yml文件中查看YAML格式的默认节点组定义集：

```
openshift_node_groups:
  - name: node-config-master    #Node group name.
    labels:
      - 'node-role.kubernetes.io/master=true'   #与节点组关联的节点标签列表。
    edits: []    #	对节点组配置的任何编辑
  - name: node-config-infra
    labels:
      - 'node-role.kubernetes.io/infra=true'
    edits: []
  - name: node-config-compute
    labels:
      - 'node-role.kubernetes.io/compute=true'
    edits: []
  - name: node-config-master-infra
    labels:
      - 'node-role.kubernetes.io/infra=true,node-role.kubernetes.io/master=true'
    edits: []
  - name: node-config-all-in-one
    labels:
      - 'node-role.kubernetes.io/infra=true,node-role.kubernetes.io/master=true,node-role.kubernetes.io/compute=true'
    edits: []
```

如果未在清单文件的[OSEv3：vars]组中设置openshift_node_groups变量，则使用这些默认值。但是，如果要设置自定义节点组，则必须在清单文件中定义整个openshift_node_groups结构，包括所有计划的节点组。

openshift_node_groups值未与默认值合并，您必须将YAML定义转换为Python字典。然后，您可以使用edits字段通过指定键值对来修改任何节点配置变量。

例如，清单文件中的以下条目定义名为node-config-master，node-config-infra和node-config-compute的组。

```
openshift_node_groups=[{'name': 'node-config-master', 'labels': ['node-role.kubernetes.io/master=true']}, {'name': 'node-config-infra', 'labels': ['node-role.kubernetes.io/infra=true']}, {'name': 'node-config-compute', 'labels': ['node-role.kubernetes.io/compute=true']}]
```

您还可以使用其他标签定义新节点组名称，清单文件中的以下条目定义名为node-config-master，node-config-infra，node-config-compute和node-config-compute-storage的组。

```
openshift_node_groups=[{'name': 'node-config-master', 'labels': ['node-role.kubernetes.io/master=true']}, {'name': 'node-config-infra', 'labels': ['node-role.kubernetes.io/infra=true']}, {'name': 'node-config-compute', 'labels': ['node-role.kubernetes.io/compute=true']}, {'name': 'node-config-compute-storage', 'labels': ['node-role.kubernetes.io/compute-storage=true']}]
```

在清单文件中设置条目时，还可以编辑节点组的ConfigMap：

- 您可以使用列表，例如修改node-config-compute以将kubeletArguments.pods-per-core设置为20：
```
openshift_node_groups=[{'name': 'node-config-master', 'labels': ['node-role.kubernetes.io/master=true']}, {'name': 'node-config-infra', 'labels': ['node-role.kubernetes.io/infra=true']}, {'name': 'node-config-compute', 'labels': ['node-role.kubernetes.io/compute=true'], 'edits': [{ 'key': 'kubeletArguments.pods-per-core','value': ['20']}]}]
```
- 您可以使用列表来修改多个键值对，例如修改node-config-compute组以将两个参数添加到kubelet：

```
openshift_node_groups=[{'name': 'node-config-master', 'labels': ['node-role.kubernetes.io/master=true']}, {'name': 'node-config-infra', 'labels': ['node-role.kubernetes.io/infra=true']}, {'name': 'node-config-compute', 'labels': ['node-role.kubernetes.io/compute=true'], 'edits': [{ 'key': 'kubeletArguments.experimental-allocatable-ignore-eviction','value': ['true']}, {'key': 'kubeletArguments.eviction-hard', 'value': ['memory.available<1Ki']}]}]
```

- 您还可以使用字典作为值，例如修改node-config-compute组以将perFSGroup设置为512Mi：

```
openshift_node_groups=[{'name': 'node-config-master', 'labels': ['node-role.kubernetes.io/master=true']}, {'name': 'node-config-infra', 'labels': ['node-role.kubernetes.io/infra=true']}, {'name': 'node-config-compute', 'labels': ['node-role.kubernetes.io/compute=true'], 'edits': [{ 'key': 'volumeConfig.localQuota','value': {'perFSGroup':'512Mi'}}]}]
```

每当运行openshift_node_group.yml playbook时，edits字段中定义的更改将更新相关的ConfigMap（本例中为node-config-compute），这最终将影响主机上节点的配置文件。

###将主机映射到节点组
要映射要用于哪个节点主机的ConfigMap，必须使用openshift_node_group_name变量将库存的[nodes]组中定义的所有主机分配给节点组。

**注:无论是使用默认节点组定义还是使用ConfigMaps还是自定义自己的集群安装，所有集群安装都需要将每个主机的openshift_node_group_name设置为节点组。**

openshift_node_group_name的值用于选择配置每个节点的ConfigMap。例如：

```
[nodes]
master[1:3].example.com openshift_node_group_name='node-config-master'
infra-node1.example.com openshift_node_group_name='node-config-infra'
infra-node2.example.com openshift_node_group_name='node-config-infra'
node1.example.com openshift_node_group_name='node-config-compute'
node2.example.com openshift_node_group_name='node-config-compute'
```

如果在openshift_node_groups中定义了其他自定义ConfigMaps，也可以使用它们。例如：

```
[nodes]
master[1:3].example.com openshift_node_group_name='node-config-master'
infra-node1.example.com openshift_node_group_name='node-config-infra'
infra-node2.example.com openshift_node_group_name='node-config-infra'
node1.example.com openshift_node_group_name='node-config-compute'
node2.example.com openshift_node_group_name='node-config-compute'
gluster[1:6].example.com openshift_node_group_name='node-config-compute-storage'
```

### 节点主机标签
您可以在群集安装期间将标签分配给节点主机。您可以使用这些标签来确定使用调度程序将pod放置到节点上。

如果要修改分配给节点主机的默认标签，则必须创建自己的自定义节点组。您无法再设置openshift_node_labels变量来更改标签。请参阅[节点组定义](https://docs.openshift.com/container-platform/3.11/install/configuring_inventory_file.html#configuring-inventory-node-group-definitions)以修改默认节点组。

除node-role.kubernetes.io/infra=true之外（使用此组的主机也称为专用基础结构节点，在配置专用基础结构节点中进一步讨论），实际的标签名称和值是任意的，可以根据您的群集要求分配。

###主机的POD调度
在安装过程中将您指定为主服务器的所有主机配置为节点。通过这样做，主服务器被配置为OpenShift SDN的一部分。您必须将主主机的条目添加到[nodes]部分：

```
[nodes]
master[1:3].example.com openshift_node_group_name='node-config-master'
```

###节点上的POD调度
默认情况下，主服务器被标记为可调度节点，因此在集群安装期间默认设置默认节点选择器。默认节点选择器在主配置文件的projectConfig.defaultNodeSelector字段中定义，以确定在放置pod时默认使用哪些节点项目。除非使用osm_default_node_selector变量覆盖，否则它将设置为node-role.kubernetes.io/compute=true。

**注：如果在安装期间接受node-role.kubernetes.io/compute=true的默认节点选择器，请确保您不仅将专用基础结构节点作为群集中定义的非主节点。在该场景中，应用程序窗格无法部署，因为在为项目调度窗格时，没有具有node-role.kubernetes.io/compute=true标签的节点可用于匹配默认节点选择器。**

###配置专用的基础设施节点
建议您在生产环境中维护专用基础结构节点，其中注册表和路由器窗格可以与用于用户应用程序的窗格分开运行。

openshift_router_selector和openshift_registry_selector Ansible设置确定放置注册表和路由器pod时使用的标签选择器。它们默认设置为node-role.kubernetes.io/infra=true：

```
# default selectors for router and registry services
# openshift_router_selector='node-role.kubernetes.io/infra=true'
# openshift_registry_selector='node-role.kubernetes.io/infra=true'
```

注册表和路由器只能在node-role.kubernetes.io/infra=true标签的节点主机上运行，​​然后将其视为专用基础结构节点.确保OpenShift Container Platform环境中至少有一个节点主机具有node-role.kubernetes.io/infra=true标签;你可以使用默认的node-config-infra来设置这个标签：

```
[nodes]
infra-node1.example.com openshift_node_group_name='node-config-infra'
```

**注：如果[nodes]部分中没有与选择器设置匹配的节点，则默认路由器和注册表将部署为失败并处于Pending状态。**

如果您不打算使用OpenShift Container Platform来管理注册表和路由器，请配置以下Ansible设置：

```
openshift_hosted_manage_registry=false
openshift_hosted_manage_router=false
```

如果使用默认registry.redhat.io以外的映像注册表，则必须在/etc/ansible/hosts文件中指定注册表。

如在Masters上配置可调度性中所述，默认情况下主主机标记为可调度。如果使用node-role.kubernetes.io/infra=true标记主主机且没有其他专用基础结构节点，则还必须将主主机标记为可调度。否则，注册表和路由器pod不能放在任何地方。您可以使用默认的node-config-master-infra节点组来实现此目的：

```
[nodes]
master.example.com openshift_node_group_name='node-config-master-infra'
```

###配置主API端口
要配置主API使用的默认端口，请在/etc/ansible/hosts文件中配置以下变量,例如：

```
openshift_master_api_port=3443
```

###配置群集预安装检查
预安装检查是一组诊断任务，作为openshift_health_checker Ansible角色的一部分运行。它们在Ansible安装OpenShift Container Platform之前运行，确保设置所需的库存值，并识别主机上可能阻止或干扰成功安装的潜在问题。

要禁用特定的安装前检查，请在清单文件中包含变量openshift_disable_check以及逗号分隔的检查名称列表。例如：

```
openshift_disable_check=memory_availability,disk_availability
```

###配置注册表位置
如果在registry.redhat.io中使用默认值以外的映像注册表，请在/etc/ansible/hosts文件中指定注册表。

```
oreg_url=example.com/openshift3/ose-${component}:${version}
openshift_examples_modify_imagestreams=true
```
默认注册表需要身份验证令牌。有关更多信息，请参阅访问和配置[Red Hat Registry](https://docs.openshift.com/container-platform/3.11/install_config/configuring_red_hat_registry.html#install-config-configuring-red-hat-registry)

例如：

```
oreg_url=example.com/openshift3/ose-${component}:${version}
oreg_auth_user=${user_name}
oreg_auth_password=${password}
openshift_examples_modify_imagestreams=true
```
###配置注册表路由
要允许用户从OpenShift Container Platform集群外部将镜像推送到内部容器映像注册表，请在/ etc/ansible/hosts文件中配置注册表路由。默认情况下，注册表路由为docker-registry-default.router.default.svc.cluster.local。例如：

```
openshift_hosted_registry_routehost=<path>
openshift_hosted_registry_routetermination=reencrypt
openshift_hosted_registry_routecertificates= "{'certfile': '<path>/org-cert.pem', 'keyfile': '<path>/org-privkey.pem', 'cafile': '<path>/org-chain.pem'}"
```
###配置路由器分片
通过向清单提供正确的数据来启用路由器分片支持。变量openshift_hosted_routers保存数据，该数据采用列表的形式。如果没有传递数据，则创建默认路由器。路由器分片有多种组合。以下示例支持单独节点上的路由器：

```
openshift_hosted_routers=[{'name': 'router1', 'certificate': {'certfile': '/path/to/certificate/abc.crt',
'keyfile': '/path/to/certificate/abc.key', 'cafile':
'/path/to/certificate/ca.crt'}, 'replicas': 1, 'serviceaccount': 'router',
'namespace': 'default', 'stats_port': 1936, 'edits': [], 'images':
'openshift3/ose-${component}:${version}', 'selector': 'type=router1', 'ports':
['80:80', '443:443']},

{'name': 'router2', 'certificate': {'certfile': '/path/to/certificate/xyz.crt',
'keyfile': '/path/to/certificate/xyz.key', 'cafile':
'/path/to/certificate/ca.crt'}, 'replicas': 1, 'serviceaccount': 'router',
'namespace': 'default', 'stats_port': 1936, 'edits': [{'action': 'append',
'key': 'spec.template.spec.containers[0].env', 'value': {'name': 'ROUTE_LABELS',
'value': 'route=external'}}], 'images':
'openshift3/ose-${component}:${version}', 'selector': 'type=router2', 'ports':
['80:80', '443:443']}]
```
###配置Red Hat Gluster存储永久存储
可以将Red Hat Gluster Storage配置为为OpenShift Container Platform提供持久存储和动态配置。它既可以在OpenShift Container Platform（融合模式）中使用容器化，也可以在自己的节点上使用非容器化（独立模式）。

###配置聚合模式
请参阅[聚合模式](https://docs.openshift.com/container-platform/3.11/install/host_preparation.html#prereq-glusterfs)特定主机准备和先决条件的注意事项。

1 在清单文件中，在[OSEv3：vars]部分中包含以下变量，并根据配置的需要进行调整：

```
[OSEv3:vars]
...
openshift_storage_glusterfs_namespace=app-storage
openshift_storage_glusterfs_storageclass=true
openshift_storage_glusterfs_storageclass_default=false
openshift_storage_glusterfs_block_deploy=true
openshift_storage_glusterfs_block_host_vol_size=100
openshift_storage_glusterfs_block_storageclass=true
openshift_storage_glusterfs_block_storageclass_default=false
```

2 在[OSEv3：children]部分添加glusterfs以启用[glusterfs]组：

```
[OSEv3:children]
masters
nodes
glusterfs
```

3 添加[glusterfs]部分，其中包含将托管GlusterFS存储的每个存储节点的条目。对于每个节点，将glusterfs_devices设置为将作为GlusterFS集群的一部分进行完全管理的原始块设备列表。必须至少列出一个设备。每个设备必须是裸设备，没有分区或LVM PV。指定变量采用以下形式：

```
<hostname_or_ip> glusterfs_devices='[ "</path/to/device1/>", "</path/to/device2>", ... ]'
```
例如：

```
[glusterfs]
node11.example.com glusterfs_devices='[ "/dev/xvdc", "/dev/xvdd" ]'
node12.example.com glusterfs_devices='[ "/dev/xvdc", "/dev/xvdd" ]'
node13.example.com glusterfs_devices='[ "/dev/xvdc", "/dev/xvdd" ]'
```

4 将[glusterfs]下列出的主机添加到[nodes]组：

```
[nodes]
...
node11.example.com openshift_node_group_name="node-config-compute"
node12.example.com openshift_node_group_name="node-config-compute"
node13.example.com openshift_node_group_name="node-config-compute"
```

###配置独立模式
1 在清单文件中，在[OSEv3：vars]部分中包含以下变量，并根据配置的需要进行调整：

```
[OSEv3:vars]
...
openshift_storage_glusterfs_namespace=app-storage
openshift_storage_glusterfs_storageclass=true
openshift_storage_glusterfs_storageclass_default=false
openshift_storage_glusterfs_block_deploy=true
openshift_storage_glusterfs_block_host_vol_size=100
openshift_storage_glusterfs_block_storageclass=true
openshift_storage_glusterfs_block_storageclass_default=false
openshift_storage_glusterfs_is_native=false
openshift_storage_glusterfs_heketi_is_native=true
openshift_storage_glusterfs_heketi_executor=ssh
openshift_storage_glusterfs_heketi_ssh_port=22
openshift_storage_glusterfs_heketi_ssh_user=root
openshift_storage_glusterfs_heketi_ssh_sudo=false
openshift_storage_glusterfs_heketi_ssh_keyfile="/root/.ssh/id_rsa"
```
2 在[OSEv3：children]部分添加glusterfs以启用[glusterfs]组：

```
[OSEv3:children]
masters
nodes
glusterfs
```
3 添加[glusterfs]部分，其中包含将托管GlusterFS存储的每个存储节点的条目。对于每个节点，将glusterfs_devices设置为将作为GlusterFS集群的一部分进行完全管理的原始块设备列表。必须至少列出一个设备。每个设备必须是裸设备，没有分区或LVM PV。另外，将glusterfs_ip设置为节点的IP地址。指定变量采用以下形式：

```
<hostname_or_ip> glusterfs_ip=<ip_address> glusterfs_devices='[ "</path/to/device1/>", "</path/to/device2>", ... ]'
```
例如：

```
[glusterfs]
gluster1.example.com glusterfs_ip=192.168.10.11 glusterfs_devices='[ "/dev/xvdc", "/dev/xvdd" ]'
gluster2.example.com glusterfs_ip=192.168.10.12 glusterfs_devices='[ "/dev/xvdc", "/dev/xvdd" ]'
gluster3.example.com glusterfs_ip=192.168.10.13 glusterfs_devices='[ "/dev/xvdc", "/dev/xvdd" ]'
```

###配置OpenShift容器注册表
可以使用安装程序部署集成的[OpenShift Container Registry](https://docs.openshift.com/container-platform/3.11/architecture/infrastructure_components/image_registry.html#integrated-openshift-registry)

###配置注册表存储
如果未使用注册表存储选项，则默认的OpenShift容器注册表是短暂的，并且当pod不再存在时，所有数据都将丢失。使用高级安装程序时，有几个选项可用于启用注册表存储：
####选项A：NFS主机组
设置以下变量时，将在群集安装期间创建NFS卷，并在[nfs]主机组中的主机上使用路径nfs_directory/volume_name。例如，使用这些选项的卷路径是/exports/registry：

```
[OSEv3:vars]

openshift_hosted_registry_storage_kind=nfs
openshift_hosted_registry_storage_access_modes=['ReadWriteMany']
openshift_hosted_registry_storage_nfs_directory=/exports
openshift_hosted_registry_storage_nfs_options='*(rw,root_squash)'
openshift_hosted_registry_storage_volume_name=registry
openshift_hosted_registry_storage_volume_size=10Gi
```
####选项B：外部NFS主机
要使用外部NFS卷，必须已存在，且存储主机上的路径为nfs_directory/volume_name。使用以下选项的远程卷路径是nfs.example.com:/exports/registry。

```
[OSEv3:vars]

openshift_hosted_registry_storage_kind=nfs
openshift_hosted_registry_storage_access_modes=['ReadWriteMany']
openshift_hosted_registry_storage_host=nfs.example.com
openshift_hosted_registry_storage_nfs_directory=/exports
openshift_hosted_registry_storage_volume_name=registry
openshift_hosted_registry_storage_volume_size=10Gi
```

####选项C：OPENSTACK平台
OpenStack存储配置必须已存在。

```
[OSEv3:vars]

openshift_hosted_registry_storage_kind=openstack
openshift_hosted_registry_storage_access_modes=['ReadWriteOnce']
openshift_hosted_registry_storage_openstack_filesystem=ext4
openshift_hosted_registry_storage_openstack_volumeID=3a650b4f-c8c5-4e0a-8ca5-eaee11f16c57
openshift_hosted_registry_storage_volume_size=10Gi
```

####选项D：AWS或另一种S3存储解决方案
简单存储解决方案（S3）存储桶必须已存在。

```
[OSEv3:vars]

#openshift_hosted_registry_storage_kind=object
#openshift_hosted_registry_storage_provider=s3
#openshift_hosted_registry_storage_s3_accesskey=access_key_id
#openshift_hosted_registry_storage_s3_secretkey=secret_access_key
#openshift_hosted_registry_storage_s3_bucket=bucket_name
#openshift_hosted_registry_storage_s3_region=bucket_region
#openshift_hosted_registry_storage_s3_chunksize=26214400
#openshift_hosted_registry_storage_s3_rootdirectory=/registry
#openshift_hosted_registry_pullthrough=true
#openshift_hosted_registry_acceptschema2=true
#openshift_hosted_registry_enforcequota=true
```
如果使用其他S3服务（如Minio或ExoScale），还要添加区域端点参数：

```
openshift_hosted_registry_storage_s3_regionendpoint=https://myendpoint.example.com/
```

####选项E：收敛模式
与配置融合模式类似，可以将Red Hat Gluster Storage配置为在初始安装群集期间为OpenShift Container Registry提供存储，以便为注册表提供冗余且可靠的存储。

1 在库存文件中，在[OSEv3：vars]部分下设置以下变量，并根据配置的需要进行调整：

```
[OSEv3:vars]
...
openshift_hosted_registry_storage_kind=glusterfs 
openshift_hosted_registry_storage_volume_size=5Gi
openshift_hosted_registry_selector='node-role.kubernetes.io/infra=true'
```

2 在[OSEv3：children]部分添加glusterfs_registry以启用[glusterfs_registry]组：

```
[OSEv3:children]
masters
nodes
glusterfs_registry
```

3 添加[glusterfs_registry]部分，其中包含将托管GlusterFS存储的每个存储节点的条目。对于每个节点，将glusterfs_devices设置为将作为GlusterFS集群的一部分进行完全管理的原始块设备列表。必须至少列出一个设备。每个设备必须是裸设备，没有分区或LVM PV。指定变量采用以下形式：

```
<hostname_or_ip> glusterfs_devices='[ "</path/to/device1/>", "</path/to/device2>", ... ]'
```

例如：

```
[glusterfs_registry]
node11.example.com glusterfs_devices='[ "/dev/xvdc", "/dev/xvdd" ]'
node12.example.com glusterfs_devices='[ "/dev/xvdc", "/dev/xvdd" ]'
node13.example.com glusterfs_devices='[ "/dev/xvdc", "/dev/xvdd" ]'
```

4 将[glusterfs_registry]下列出的主机添加到[nodes]组：

```
[nodes]
...
node11.example.com openshift_node_group_name="node-config-infra"
node12.example.com openshift_node_group_name="node-config-infra"
node13.example.com openshift_node_group_name="node-config-infra"
```

####选项F：GOOGLE计算引擎（GCE）上的GOOGLE云存储（GCS）桶
GCS存储桶必须已存在。

```
[OSEv3:vars]

openshift_hosted_registry_storage_provider=gcs
openshift_hosted_registry_storage_gcs_bucket=bucket01
openshift_hosted_registry_storage_gcs_keyfile=test.key
openshift_hosted_registry_storage_gcs_rootdirectory=/registry
```

####选项G：带有VSPHERE云提供商（VCP）的VSPHERE变量
必须使用OpenShift Container Platform节点可访问的数据存储配置vSphere Cloud Provider。将vSphere卷用于注册表时，必须将存储访问模式设置为ReadWriteOnce，并将副本计数设置为1：

```
[OSEv3:vars]

openshift_hosted_registry_storage_kind=vsphere
openshift_hosted_registry_storage_access_modes=['ReadWriteOnce']
openshift_hosted_registry_storage_annotations=['volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/vsphere-volume']
openshift_hosted_registry_replicas=1
```

###配置全局代理选项
如果您的主机需要使用HTTP或HTTPS代理才能连接到外部主机，则必须配置许多组件才能使用代理，包括主服务器，Docker和构建。节点服务仅连接到不需要外部访问的主API，因此不需要配置为使用代理。

为了简化此配置，可以在群集或主机级别指定以下Ansible变量，以在整个环境中统一应用这些设置。

有关如何为构建定义代理环境的更多信息，请参阅配置[全局构建默认值和覆盖](https://docs.openshift.com/container-platform/3.11/install_config/build_defaults_overrides.html#install-config-build-defaults-overrides)

###配置防火墙
**注：如果要更改默认防火墙，请确保群集中的每个主机使用相同的防火墙类型以防止出现不一致。不要将firewalld与Atomic Host上安装的OpenShift Container Platform一起使用。原子主机不支持firewalld。虽然iptables是默认防火墙，但建议将firewalld用于新安装。**

OpenShift Container Platform使用iptables作为默认防火墙，但您可以将群集配置为在安装过程中使用firewalld。

由于iptables是默认防火墙，因此OpenShift Container Platform旨在自动配置它。但是，如果未正确配置，iptables规则可能会破坏OpenShift容器平台。firewalld的优点包括允许多个对象安全地共享防火墙规则。

要使用firewalld作为OpenShift Container Platform安装的防火墙，请在安装时将os_firewall_use_firewalld变量添加到Ansible主机文件中的配置变量列表中：

```
[OSEv3:vars]
os_firewall_use_firewalld=True 
```
### 配置Session选项
OAuth配置中的会话选项可在清单文件中进行配置。默认情况下，Ansible使用生成的身份验证和加密机密填充sessionSecretsFile，以便其他人可以解码由一个主服务器生成的会话。默认位置为/etc/origin/master/session-secrets.yaml，如果在所有主机上删除，则会重新创建此文件。

您可以使用openshift_master_session_name和openshift_master_session_max_seconds设置会话名称和最大秒数：

```
openshift_master_session_name=ssn
openshift_master_session_max_seconds=3600
```

如果提供，openshift_master_session_auth_secrets和openshift_master_encryption_secrets必须相等。

对于openshift_master_session_auth_secrets，用于使用HMAC验证会话，建议使用32或64字节的机密：

```
openshift_master_session_auth_secrets=['DONT+USE+THIS+SECRET+b4NV+pmZNSO']
```

对于openshift_master_encryption_secrets，用于加密会话，秘密长度必须为16个，24个或32个字符，以选择AES-128，AES-192或AES-256：

```
openshift_master_session_encryption_secrets = [ 'DONT + USE + THIS + SECRET + b4NV + pmZNSO']
```

###配置自定义证书
可以在群集安装期间部署OpenShift Container Platform API和Web控制台的公共主机名的自定义服务证书，并且可以在清单文件中进行配置。

**注：**为与publicMasterURL关联的主机名配置自定义证书，您将其设置为openshift_master_cluster_public_hostname参数值。对与masterURL关联的主机名使用自定义服务证书（openshift_master_cluster_hostname）会导致TLS错误，因为基础架构组件尝试使用内部masterURL主机联系主API。

可以使用openshift_master_named_certificates集群变量配置证书和密钥文件路径：

```
openshift_master_named_certificates=[{"certfile": "/path/to/custom1.crt", "keyfile": "/path/to/custom1.key", "cafile": "/path/to/custom-ca1.crt"}]
```

文件路径必须是运行Ansible的系统的本地路径。证书将复制到主主机，并部署在/etc/origin/ master/ named_certificates/目录中。

Ansible检测证书的公共名称和主题备用名称。设置openshift_master_named_certificates时，可以通过提供“names”键来覆盖检测到的名称：

```
openshift_master_named_certificates=[{"certfile": "/path/to/custom1.crt", "keyfile": "/path/to/custom1.key", "names": ["public-master-host.com"], "cafile": "/path/to/custom-ca1.crt"}]
```

使用openshift_master_named_certificates配置的证书缓存在主服务器上，这意味着每个附加的Ansible使用不同的证书集运行会导致所有以前部署的证书保留在主主机和主配置文件中。如果要使用提供的值（或无值）覆盖openshift_master_named_certificates，请指定openshift_master_overwrite_named_certificates群集变量：

```
openshift_master_overwrite_named_certificates=true
```

有关更完整的示例，请考虑清单文件中的以下群集变量：

```
openshift_master_cluster_method=native
openshift_master_cluster_hostname=lb-internal.openshift.com
openshift_master_cluster_public_hostname=custom.openshift.com
```

要在随后的Ansible运行中覆盖证书，请设置以下参数值：

```
openshift_master_named_certificates=[{"certfile": "/root/STAR.openshift.com.crt", "keyfile": "/root/STAR.openshift.com.key", "names": ["custom.openshift.com"]}]
openshift_master_overwrite_named_certificates=true
```

###配置证书有效性
默认情况下，用于管理etcd，master和kubelet的证书将在两到五年后过期。可以使用以下变量在安装期间配置自动生成的注册表，CA，节点和主证书的有效性（到期之前的天数）（显示默认值）：

```
[OSEv3:vars]

openshift_hosted_registry_cert_expire_days=730
openshift_ca_cert_expire_days=1825
openshift_node_cert_expire_days=730
openshift_master_cert_expire_days=730
etcd_ca_default_days=1825
```

###配置群集监视
Prometheus Cluster Monitoring设置为自动部署。要阻止其自动部署，请设置以下内容：

```
[OSEv3:vars]

openshift_cluster_monitoring_operator_install=false
```

###配置群集指标
群集指标未设置为自动部署。设置以下内容以在群集安装期间启用群集指标：

```
[OSEv3:vars]

openshift_metrics_install_metrics=true
```

可以使用openshift_metrics_hawkular_hostname Ansible变量在群集安装期间设置度量标准公用URL，默认为：

**https://hawkular-metrics.{{openshift_master_default_subdomain}}/hawkular/metrics**

如果更改此变量，请确保可通过路由器访问主机名。

**openshift_metrics_hawkular_hostname=hawkular-metrics.{{openshift_master_default_subdomain}}**

###配置度量标准存储
必须设置openshift_metrics_cassandra_storage_type变量才能持久的标准存储。如果未设置openshift_metrics_cassandra_storage_type，则群集指标数据将存储在emptyDir卷中，当Cassandra pod终止时，该卷将被删除。

在群集安装期间，有三个选项可用于启用群集指标存储：

####选项A：动态
如果您的OpenShift Container Platform环境支持云提供商的动态卷配置，请使用以下变量：

```
[OSEv3:vars]

openshift_metrics_cassandra_storage_type=dynamic
```

如果存在多个默认动态调配卷类型（例如gluster-storage和glusterfs-storage-block），则可以按变量指定预配置卷类型。使用以下变量：

```
[OSEv3:vars]

openshift_metrics_cassandra_storage_type=pv
openshift_metrics_cassandra_pvc_storage_class_name=glusterfs-storage-block
```

####选项B：NFS主机组
设置以下变量时，将在群集安装期间创建NFS卷，并在[nfs]主机组中的主机上使用路径nfs_directory/ <volume_name>。例如，使用这些选项的卷路径是/exports/metrics：

```
[OSEv3:vars]

openshift_metrics_storage_kind=nfs
openshift_metrics_storage_access_modes=['ReadWriteOnce']
openshift_metrics_storage_nfs_directory=/exports
openshift_metrics_storage_nfs_options='*(rw,root_squash)'
openshift_metrics_storage_volume_name=metrics
openshift_metrics_storage_volume_size=10Gi
```

####选项C：外部NFS主机
要使用外部NFS卷，必须已存在，且存储主机上的路径为nfs_directory/<volume_name>。

```
[OSEv3:vars]

openshift_metrics_storage_kind=nfs
openshift_metrics_storage_access_modes=['ReadWriteOnce']
openshift_metrics_storage_host=nfs.example.com
openshift_metrics_storage_nfs_directory=/exports
openshift_metrics_storage_volume_name=metrics
openshift_metrics_storage_volume_size=10Gi
```

**使用NFS升级或安装OPENSHIFT容器平台**

建议不要将NFS用于核心OpenShift Container Platform组件，因为NFS（和NFS协议）无法提供构成OpenShift Container Platform基础结构的应用程序所需的适当一致性。

因此，安装程序和更新游戏手册需要一个选项，以便将NFS与核心基础结构组件一起使用。 
	
	#Enable unsupported configurations, things that will yield a partially
	# functioning cluster but would not be supported for production use
	#openshift_enable_unsupported_configurations=false
	
如果在升级或安装群集时看到以下消息，则需要执行其他步骤。

```
TASK [Run variable sanity checks] **********************************************
fatal: [host.example.com]: FAILED! => {"failed": true, "msg": "last_checked_host: host.example.com, last_checked_var: openshift_hosted_registry_storage_kind;nfs is an unsupported type for openshift_hosted_registry_storage_kind. openshift_enable_unsupported_configurations=True mustbe specified to continue with this configuration."}
```
在Ansible清单文件中，指定以下参数：

```
[OSEv3:vars]
openshift_enable_unsupported_configurations=True
```

###配置日志存储
必须设置openshift_logging_es_pvc_dynamic变量才能使用持久存储进行日志记录。如果未设置openshift_logging_es_pvc_dynamic，则群集日志记录数据将存储在emptyDir卷中，该卷将在Elasticsearch pod终止时删除。

在群集安装期间启用群集日志记录存储有三个选项：
####选项A：动态
如果您的OpenShift Container Platform环境具有动态卷配置，则可以通过云提供商或独立存储提供商进行配置。例如，云提供商可以在GCE上拥有一个带有供应商kubernetes.io/gce-pd的StorageClass,而像GlusterFS这样的独立存储提供商可以拥有一个带有供应商kubernetes.io/glusterfs的StorageClass。在任何一种情况下，请使用以下变量：

```
[OSEv3:vars]

openshift_logging_es_pvc_dynamic=true
```

如果存在多个默认动态调配卷类型（例如gluster-storage和glusterfs-storage-block），则可以按变量指定预配置卷类型。使用以下变量：

```
[OSEv3:vars]

openshift_logging_elasticsearch_storage_type=pvc
openshift_logging_es_pvc_storage_class_name=glusterfs-storage-block
```

####选项B：NFS主机组
设置以下变量时，将在群集安装期间创建NFS卷，并在[nfs]主机组中的主机上使用路径nfs_directory>/ <volume_name>。例如，使用这些选项的卷路径是/exports/logging：

```
[OSEv3:vars]

openshift_logging_storage_kind=nfs
openshift_logging_storage_access_modes=['ReadWriteOnce']
openshift_logging_storage_nfs_directory=/exports
openshift_logging_storage_nfs_options='*(rw,root_squash)'
openshift_logging_storage_volume_name=logging
openshift_logging_storage_volume_size=10Gi
```

####选项C：外部NFS主机
要使用外部NFS卷，必须已存在，且存储主机上的路径为nfs_directory/<volume_name>。

```
[OSEv3:vars]

openshift_logging_storage_kind=nfs
openshift_logging_storage_access_modes=['ReadWriteOnce']
openshift_logging_storage_host=nfs.example.com
openshift_logging_storage_nfs_directory=/exports
openshift_logging_storage_volume_name=logging
openshift_logging_storage_volume_size=10Gi
```

远程卷路径使用以下选项的 nfs.example.com:/exports/logging

**使用NFS升级或安装OPENSHIFT容器平台**

建议不要将NFS用于核心OpenShift Container Platform组件，因为NFS（和NFS协议）无法提供构成OpenShift Container Platform基础结构的应用程序所需的适当一致性。

因此，安装程序和更新游戏手册需要一个选项，以便将NFS与核心基础结构组件一起使用。 
	
	#Enable unsupported configurations, things that will yield a partially
	# functioning cluster but would not be supported for production use
	#openshift_enable_unsupported_configurations=false
	
如果在升级或安装群集时看到以下消息，则需要执行其他步骤。

```
TASK [Run variable sanity checks] **********************************************
fatal: [host.example.com]: FAILED! => {"failed": true, "msg": "last_checked_host: host.example.com, last_checked_var: openshift_hosted_registry_storage_kind;nfs is an unsupported type for openshift_hosted_registry_storage_kind. openshift_enable_unsupported_configurations=True mustbe specified to continue with this configuration."}
```
在Ansible清单文件中，指定以下参数：

```
[OSEv3:vars]
openshift_enable_unsupported_configurations=True
```
###定制Service Catalog选项
安装期间默认使用Service Catalog。启用服务代理允许您使用目录注册服务代理。启用服务目录后，也会安装OpenShift Ansible代理和模板服务代理;如果禁用服务目录，则不会安装OpenShift Ansible代理和模板服务代理。

要禁用服务目录的自动部署，请在清单文件中设置以下群集变量：

```
openshift_enable_service_catalog=false
```

如果您使用自己的注册表，则必须添加：

- openshift_service_catalog_image_prefix: 拉动服务目录镜像时，强制使用特定前缀（例如，registry）。您必须提供完整的注册表名称，直到图像名称。
- openshift_service_catalog_image_version: 拉动服务目录镜像时，强制使用特定的镜像版本。

例如：

```
openshift_service_catalog_image="docker-registry.default.example.com/openshift/ose-service-catalog:${version}"
openshift_service_catalog_image_prefix="docker-registry-default.example.com/openshift/ose-"
openshift_service_catalog_image_version="v3.9.30"
template_service_broker_selector={"role":"infra"}
```

###配置OpenShift Ansible Broker
安装期间默认启用OpenShift Ansible代理（OAB）。如果您不想安装OAB，请在清单文件中将ansible_service_broker_install参数值设置为false：

```
ansible_service_broker_install=false
```

####为OpenShift Ansible Broker配置持久性存储
OAB部署了自己的etcd实例，与其他OpenShift Container Platform集群使用的etcd分开。OAB的etcd实例需要使用持久卷（PV）进行单独存储才能运行。如果没有PV可用，则etcd将等待直到PV满足为止。 OAB应用程序将进入CrashLoop状态，直到其etcd实例可用。

一些Ansible playbook捆绑包（APB）也需要PV用于自己的使用才能进行部署。例如，每个数据库APB都有两个计划：开发计划使用临时存储而不需要PV，而生产计划是持久的并且确实需要PV。

**要为OAB配置持久存储：**

1 在存储文件中，将nfs添加到[OSEv3：children]部分以启用[nfs]组：

```
[OSEv3:children]
masters
nodes
nfs
```

2 添加[nfs]组部分并添加将成为NFS主机的系统的主机名：

```
[nfs]
master1.example.com
```

3 在[OSEv3：vars]部分中添加以下内容：

```
openshift_hosted_etcd_storage_kind=nfs
openshift_hosted_etcd_storage_nfs_options="*(rw,root_squash,sync,no_wdelay)"
openshift_hosted_etcd_storage_nfs_directory=/opt/osev3-etcd 
openshift_hosted_etcd_storage_volume_name=etcd-vol2 
openshift_hosted_etcd_storage_access_modes=["ReadWriteOnce"]
openshift_hosted_etcd_storage_volume_size=1G
openshift_hosted_etcd_storage_labels={'storage': 'etcd'}
```

这些设置创建在群集安装期间附加到OAB的etcd实例的持久卷。

**为本地APB开发配置OPENSHIFT ANSIBLE BROKER**
为了使用OpenShift容器注册表与OAB一起进行APB开发，必须定义OAB可以访问的镜像白名单。如果未定义白名单，则代理将忽略APB，并且用户将看不到任何可用的APB。

默认情况下，白名单为空，这样用户无法在没有集群管理员配置代理的情况下将APB映像添加到代理。将所有以-apb结尾的镜像列入白名单：

在您的库存文件中，将以下内容添加到[OSEv3：vars]部分：

```
ansible_service_broker_local_registry_whitelist=['.*-apb$']

```
### 配置模板Service Broker
安装期间默认启用模板Service Broker（TSB）。
如果您不想安装TSB，请将template_service_broker_install参数值设置为false：

```
template_service_broker_install=false
```

要配置TSB，必须将一个或多个项目定义为代理的源名称空间，以便将模板和图像流加载到服务目录中。通过修改清单文件的[OSEv3：vars]部分中的以下内容来设置源项目：

```
openshift_template_service_broker_namespaces=['openshift','myproject']
```

默认情况下，TSB使用nodeselector {“node-role.kubernetes.io/infra":"true”}来部署其pod。您可以在清单文件的[OSEv3：vars]部分中设置不同的nodeselector：

```
template_service_broker_selector={"node-role.kubernetes.io/infra":"true"}
```

###配置自定义Web控制台

以下Ansible变量设置用于自定义Web控制台的主配置选项。有关这些自定义选项的更多详细信息，请参阅[自定义Web控制台。
](https://docs.openshift.com/container-platform/3.11/install_config/web_console_customization.html#install-config-web-console-customization)

###配置群集控制台
群集控制台是一个额外的Web界面，如Web控制台，但专注于管理任务。群集控制台支持许多与Web控制台相同的常见OpenShift Container Platform资源,但它还允许您查看有关群集的度量标准并管理群集范围的资源，例如节点，持久卷，群集角色和自定义资源定义。
##安装OpenShift Container Platform
###先决条件
- 配置好系统环境
- 如果你的集群很大，查看[Scaling and Performance Guide](https://docs.openshift.com/container-platform/3.11/scaling_performance/install_practices.html#scaling-performance-install-best-practices)以优化安装时间
- 准备的的主机。此过程包括验证每个组件类型的系统和环境要求，安装和配置docker服务以及安装Ansible 2.6或更高版本。您必须安装Ansible才能运行安装playbooks。
- 配置你的存储文件，定义您的环境和OpenShift Container Platform集群配置。初始安装和将来的群集升级均基于此清单文件。
- 如果要在Red Hat Enterprise Linux上安装OpenShift Container Platform，请确定是否要使用RPM或系统容器安装方法。RHEL Atomic Host系统需要系统容器方法。

###运行RPM-based安装
基于RPM的安装程序使用通过RPM软件包安装的Ansible来运行本地主机上可用的playbooks和配置文件。

**注意：注意不用使用nohup运行，这会导致要创建但未关闭的文件描述符。因此，系统可能会耗尽文件以打开并且剧本失败。**

####运行RPM-based安装

1 切换到playbook目录并运行prerequisites.yml playbook。此playbook安装所需的软件包（如果有），并修改容器运行时。除非您需要配置容器运行时，否则在第一次部署集群之前，只运行此playbook一次：

```
$ cd /usr/share/ansible/openshift-ansible
$ ansible-playbook [-i /path/to/inventory] \    #如果你的存储文件路径不是/etc/ansible/hosts,可以通过-i指定
  playbooks/prerequisites.yml
```

2 切换到playbook目录并运行deploy_cluster.yml playbook以启动集群安装：

```
$ cd /usr/share/ansible/openshift-ansible
$ ansible-playbook [-i /path/to/inventory] \    #如果你的存储文件路径不是/etc/ansible/hosts,可以通过-i指定
    playbooks/deploy_cluster.yml
```

###运行容器化安装程序
openshift3/ose-ansible镜像是OpenShift Container Platform安装程序的容器化版本。容器化安装与RPM-based安装提供同样的功能，但它运行在容器化的环境中，提供所有依赖项，而不是直接安装在主机上。使用它的唯一要求是运行容器的能力。

####作为系统容器运行安装程序
安装程序镜像可用作系统容器。系统容器存储在传统的docker服务之外并在其外运行。这样可以从其中一个目标主机运行安装程序镜像，而无需担心主机上的安装重新启动docker。

要使用Atomic CLI将安装程序作为一次运行系统容器运行，请以root用户身份执行以下步骤：

1 运行prerequisites.yml playbook：

```
# atomic install --system \
    --storage=ostree \
    --set INVENTORY_FILE=/path/to/inventory \   #在库存文件的本地主机上指定位置。
    --set PLAYBOOK_FILE=/usr/share/ansible/openshift-ansible/playbooks/prerequisites.yml \
    --set OPTS="-v" \
    registry.redhat.io/openshift3/ose-ansible:v3.11
```

此命令通过使用指定的清单文件和root用户的SSH配置来运行一组prerequiste任务。

2 运行deploy_cluster.yml playbook：

```
# atomic install --system \
    --storage=ostree \
    --set INVENTORY_FILE=/path/to/inventory \   #在库存文件的本地主机上指定位置。
    --set PLAYBOOK_FILE=/usr/share/ansible/openshift-ansible/playbooks/deploy_cluster.yml \
    --set OPTS="-v" \
    registry.redhat.io/openshift3/ose-ansible:v3.11
```

此命令使用指定的清单文件和root用户的SSH配置启动集群安装。它将输出记录在终端上，并将其保存在/var/log/ansible.log文件中。第一次运行此命令时，映像将导入到OSTree存储中（系统容器使用此命令而不是docker守护程序存储）。在后续运行中，它重用存储的镜像。

#### 运行其他playbooks
您可以使用PLAYBOOK_FILE环境变量来指定要使用容器化安装程序运行的其他playbook。PLAYBOOK_FILE的默认值是/usr/share/ansible/openshift-ansible/playbooks/deploy_cluster.yml，它是主群集安装playbook，但您可以将其设置为容器内另一个playbook的路径。

例如，要在安装之前运行安装前检查playbook，请使用以下命令：

```
# atomic install --system \
    --storage=ostree \
    --set INVENTORY_FILE=/path/to/inventory \
    --set PLAYBOOK_FILE=/usr/share/ansible/openshift-ansible/playbooks/openshift-checks/pre-install.yml \     #将PLAYBOOK_FILE设置为从playbooks /目录开始的playbook的完整路径。 Playbooks与基于RPM的安装程序位于相同的位置。
    --set OPTS="-v" \  #设置OPTS以向ansible-playbook添加命令行选项。
    registry.redhat.io/openshift3/ose-ansible:v3.11
```

####运行容器化安装
安装程序镜像也可以作为docker容器运行的任何docker可运行的平台。
至少，当将安装程序作为docker容器运行时，您必须提供：

- SSH密钥，以便Ansible可以访问您的主机。
- Ansible库存文件
- Ansible playbook位置针对该库存运行。

以下是如何通过docker运行安装的示例，该安装程序必须由具有docker访问权限的非root用户运行：

1 首先运行prerequisites.yml playbook:

```
docker run -t -u `id -u` \ 
    -v $HOME/.ssh/id_rsa:/opt/app-root/src/.ssh/id_rsa:Z \ 
    -v $HOME/ansible/hosts:/tmp/inventory:Z \ 
    -e INVENTORY_FILE=/tmp/inventory \ 
    -e PLAYBOOK_FILE=playbooks/prerequisites.yml \ 
    -e OPTS="-v" \ 
    registry.redhat.io/openshift3/ose-ansible:v3.11
```

2 之后运行deploy_cluster.yml playbook开始集群安装

```
$ docker run -t -u `id -u` \
    -v $HOME/.ssh/id_rsa:/opt/app-root/src/.ssh/id_rsa:Z \
    -v $HOME/ansible/hosts:/tmp/inventory:Z \
    -e INVENTORY_FILE=/tmp/inventory \
    -e PLAYBOOK_FILE=playbooks/deploy_cluster.yml \
    -e OPTS="-v" \
    registry.redhat.io/openshift3/ose-ansible:v3.11
```

####运行OPENSTACK安装PLAYBOOK
要在现有OpenStack安装上安装OpenShift Container Platform，请使用OpenStack playbook。
要运行该playbook，请运行以下命令：

```
$ ansible-playbook --user openshift \
  -i openshift-ansible/playbooks/openstack/inventory.py \
  -i inventory \
  openshift-ansible/playbooks/openstack/openshift-cluster/provision_install.yml
```

#### 关于安装playbooks
安装程序使用模块化的playbooks，以便管理员可以根据需要安装特定的组件。通过分解角色和playbook，可以更好地定位临时管理任务。这样可以提高安装过程中的控制水平，从而节省时间。

主要安装手册/usr/share/ansible/openshift-ansible/playbooks/deploy_cluster.yml按特定顺序运行一组单独的组件手册，并且安装程序最后报告您经历的阶段。如果安装失败，将通知您哪个阶段失败以及Ansible运行中的错误。

###重试安装
如果Ansible安装程序失败，您仍然可以安装OpenShift Container Platform：

1. 查看已知问题以检查任何特定说明或解决方法
2. 解决安装中的错误
3. 确定是否需要卸载或者重新安装
	- 如果您未修改SDN配置或生成新证书，请重试安装。
	- 如果您修改了SDN配置，生成了新证书，或者安装程序再次失败，则必须重新安装干净的操作系统或卸载并重新安装。
	-  如果您使用虚拟机，请从一个新的镜像开始，或者再次卸载并安装。
	-  如果您使用裸机，请再次卸载并安装。

4. 重试安装：
	- 你可以再一次运行deploy_cluster.yml
	- 您可以运行剩余的单个安装手册。
	
	如果你想只运行其余的playbook，首先要为失败的阶段运行playbook，然后按顺序运行剩余的每本playbook。使用以下命令运行每个playbook：

```
# ansible-playbook [-i /path/to/inventory] <playbook_file_location>
```

###验证安装
安装完成后验证：

1 验证主服务器是否已启动且节点已注册并处于“就绪”状态。在主控主机上，以root身份运行以下命令：

```
# oc get nodes
NAME                   STATUS    ROLES     AGE       VERSION
master.example.com     Ready     master    7h        v1.9.1+a0ce1bc657
node1.example.com      Ready     compute   7h        v1.9.1+a0ce1bc657
node2.example.com      Ready     compute   7h        v1.9.1+a0ce1bc657
```
2 要验证是否正确安装了Web控制台，请使用主主机名和Web控制台端口号通过Web浏览器访问Web控制台。

例如，对于主机名为master.openshift.com且使用默认端口8443的主控主机，Web控制台URL为https://master.openshift.com:8443/console。

###验证多个ETCD主机
如果您安装了多个etcd主机：

1 首先，验证是否安装了提供etcdctl命令的etcd包：

```
# yum install etcd
```

2 在主控主机上，验证etcd群集运行状况，替换以下内容中的etcd主机的FQDN：

```
# etcdctl -C \
    https://etcd1.example.com:2379,https://etcd2.example.com:2379,https://etcd3.example.com:2379 \
    --ca-file=/etc/origin/master/master.etcd-ca.crt \
    --cert-file=/etc/origin/master/master.etcd-client.crt \
    --key-file=/etc/origin/master/master.etcd-client.key cluster-health
```

3 还要验证成员列表是否正确：

```
# etcdctl -C \
    https://etcd1.example.com:2379,https://etcd2.example.com:2379,https://etcd3.example.com:2379 \
    --ca-file=/etc/origin/master/master.etcd-ca.crt \
    --cert-file=/etc/origin/master/master.etcd-client.crt \
    --key-file=/etc/origin/master/master.etcd-client.key member list
```

###使用HAPROXY验证多个主人
如果您使用HAProxy作为负载均衡器安装了多个主服务器，请打开以下URL并检查HAProxy的状态：

```
http://<lb_hostname>:9000 #提供清单文件的[lb]部分中列出的负载均衡器主机名。
```

### 可选择保护构建
运行docker build是一个特权进程，因此容器对某些节点的访问权限比某些多租户环境中可接受的更多。
如果您不信任您的用户，则可以在安装后配置更安全的选项。在群集上禁用Docker构建，并要求用户在群集外部构建镜像。

### 已知问题
- 在多个主群集中进行故障转移时，控制器管理器可能会进行过度校正，从而导致系统运行的容器数量超出预期。但是，这是一个瞬态事件，系统会随着时间的推移自行纠正。
- 由于已知问题，在运行安装后，如果为任何组件配置了NFS卷，则无论是否将其组件部署到NFS卷，都可能会创建以下目录：
	- /exports/logging-es
	- /exports/logging-es-ops/
	- /exports/metrics/
	- /exports/prometheus
	- /exports/prometheus-alertbuffer/
	- /exports/prometheus-alertmanager/
在安装完成后你可以根据需要决定是否删除它们。

## 卸载OpenShift Container Platform
您可以通过运行uninstall.yml playbook来卸载群集中的OpenShift Container Platform主机。删除Ansible安装的OpenShift Container Platform内容，包括：

- Configuration
- Containers
- Default templates and image streams
- Images
- RPM packages

该playbook删除您在运行该剧本时指定的库存文件中定义的任何主机的内容。

###卸载OpenShift Container Platform集群
要在群集中的所有主机上卸载OpenShift Container Platform，请切换到playbook目录并使用最近使用的清单文件运行playbook：

```
# ansible-playbook [-i /path/to/file] \   #如果您的清单文件不在/etc/ansible/hosts目录中，请指定-i和清单文件的路径。
    /usr/share/ansible/openshift-ansible/playbooks/adhoc/uninstall.yml
```

###卸载节点
要使用uninstall.yml playbook从特定主机卸载节点组件，同时仅保留其余主机和群集：

1 按照[删除节点](https://docs.openshift.com/container-platform/3.11/admin_guide/manage_nodes.html#deleting-nodes)中的步骤从群集中删除节点对象。

2 创建仅引用这些主机的其他清单文件。例如，要仅从一个节点删除内容：

```
[OSEv3:children]
nodes    #仅包括适用于要卸载的主机的部分。

[OSEv3:vars]
ansible_ssh_user=root
openshift_deployment_type=openshift-enterprise

[nodes]
node3.example.com openshift_node_group_name='node-config-infra'  #仅包括要卸载的主机。
```

3 切换到playbook目录并运行uninstall.yml playbook：

```
# ansible-playbook -i /path/to/new/file \   #指定新库存文件的路径
    /usr/share/ansible/openshift-ansible/playbooks/adhoc/uninstall.yml
```



